{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0d2e007",
   "metadata": {},
   "source": [
    "# Finding Label Errors in Token Classification Datasets \n",
    "\n",
    "This tutorial shows how you can use cleanlab to find potential label errors in text datasets used for the NLP task of token classification. In token-classification, our data consists of a bunch of sentences (aka documents) in which every token (aka word) is labeled with one of K classes, and we train models to predict the class of each token in a new sentence. Example applications include part-of-speech-tagging or entity recognition, which is the focus on this tutorial. Here, we use CONLL-2003 named entity recognition dataset which contains 20,718 sentences and 301,361 tokens, where each token is labeled with one of the following classes:\n",
    "\n",
    "- LOC (location entity)\n",
    "- PER (person entity)\n",
    "- ORG (organization entity)\n",
    "- MISC (miscellaneous other type of entity)\n",
    "- O (other type of word that does not correspond to an entity)\n",
    "\n",
    "**Overview of what we'll do in this tutorial:** \n",
    "- Identify potential token label issues using cleanlab's `token_classification.filter.find_label_issues` method. \n",
    "- Rank sentences using cleanlab's `token_classification.rank.get_label_quality_scores` method. \n",
    "- TODO: (Clean Learning) Train a more robust model by removing problematic sentences. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da020bc",
   "metadata": {},
   "source": [
    "## 1. Install required dependencies and download data\n",
    "\n",
    "You can use `pip` to install all packages required for this tutorial as follows: \n",
    "\n",
    "    !pip install cleanlab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8a08e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc https://data.deepai.org/conll2003.zip && mkdir data \n",
    "!unzip conll2003.zip -d data/ && rm conll2003.zip \n",
    "!wget -nc 'https://cleanlab-public.s3.amazonaws.com/TokenClassification/pred_probs.npz' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439b0305",
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "# Package installation (hidden on docs website).\n",
    "# Package versions used: cleanlab==2.0.0 numpy==1.16.6 \n",
    "# ericwang/cleanlab -b token_classification for now \n",
    "\n",
    "dependencies = [\"cleanlab\"]\n",
    "\n",
    "if \"google.colab\" in str(get_ipython()):  # Check if it's running in Google Colab\n",
    "    %pip install git+https://github.com/cleanlab/cleanlab.git@0b4e9089d509d3d17ec026e59c58776a85453b2c\n",
    "    cmd = ' '.join([dep for dep in dependencies if dep != \"cleanlab\"])\n",
    "    %pip install $cmd\n",
    "else:\n",
    "    missing_dependencies = []\n",
    "    for dependency in dependencies:\n",
    "        try:\n",
    "            __import__(dependency)\n",
    "        except ImportError:\n",
    "            missing_dependencies.append(dependency)\n",
    "\n",
    "    if len(missing_dependencies) > 0:\n",
    "        print(\"Missing required dependencies:\")\n",
    "        print(*missing_dependencies, sep=\", \")\n",
    "        print(\"\\nPlease install them before running the rest of this notebook.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1349304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cleanlab.token_classification.filter import find_label_issues \n",
    "from cleanlab.token_classification.rank import get_label_quality_scores, issues_from_scores \n",
    "from cleanlab.internal.token_classification_utils import get_sentence, filter_sentence, mapping \n",
    "from cleanlab.token_classification.summary import display_issues, common_label_issues, filter_by_token \n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad75b45",
   "metadata": {},
   "source": [
    "## 2. Get `pred_probs`, `labels`  and read file \n",
    "\n",
    "`pred_probs` are out-of-sample model-predicted probabilities of the CoNLL-2003 dataset (including training, development, and testing dataset), obtained via cross-validation. To detect potential labels issues, we first get `pred_probs` and `labels`, which are both in nested-list format, such that: \n",
    "\n",
    "- `pred_probs` is a list of `np.arrays`, such that `pred_probs[i]` is the model-predicted probabilities for the tokens in the i'th sentence, and has shape `(N_i, K)`, where `N_i` is the number of word-level tokens of the `i`'th sentence. Each row of the matrix corresponds to a token `t` and contains the model-predicted probabilities that `t` belongs to each possible class, for each of the K classes. The columns must be ordered such that the probabilities correspond to class 0, 1, ..., K-1. \n",
    "        \n",
    "- `labels` is a list of lists, such that `labels[i]` is a list of given token labels of the `i`'th sentence. For dataset with K classes, labels must be in 0, 1, ..., K-1. All the classes (0, 1, ..., and K-1) MUST be present in ``labels[i]`` for some ``i``. \n",
    "\n",
    "Here, indicies are a tuple `(i, j)` unless otherwise specified, which refers to the `j`'th word-level token of the `i`'th sentence. Given that each sentence has different number of tokens, we store `pred_probs` and `labels` as `.npz` files, which can be easily converted to dictionaries. Use `read_npz` to retrieve `pred_probs` and `labels` in nested-list format. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4ebdc0",
   "metadata": {},
   "source": [
    "<details><summary>Below is the code used to read the `.npz` file. </summary> \n",
    "    \n",
    "    def read_npz(filepath): \n",
    "        data = dict(np.load(filepath)) \n",
    "        data = [data[str(i)] for i in range(len(data))] \n",
    "        return data \n",
    "\n",
    "</details> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9d59a0",
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "def read_npz(filepath): \n",
    "    data = dict(np.load(filepath)) \n",
    "    data = [data[str(i)] for i in range(len(data))] \n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519cb80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs = read_npz('pred_probs.npz') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8136f37",
   "metadata": {},
   "source": [
    "As shown above, `pred_probs` is a list of np.array. In our example, the first sentence has 9 given tokens and 5 class names. \n",
    "\n",
    "Given that we would like to visualize the results, we first read the files. We obtain the sentences from the original files to display the word-level token label issues in context. `given_words` contains the word-level tokens in the dataset such that `given_words[i]` is a list of words of the `i`'th sentence; `given_labels` contains the given labels in the dataset such that `given_labels[i]` is a list of labels of the `i`th sentence. Note that in CoNLL-2003, the `B-` and `I-` prefixes indicates whether the tokens are at the start of an entity, which are ignored in this tutorial. Therefore, we have two sets of entities: \n",
    "\n",
    "    given_entities = ['O', 'B-MISC', 'I-MISC', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'] \n",
    "     \n",
    "and \n",
    "\n",
    "    merge_entities = ['O', 'MISC', 'PER', 'ORG', 'LOC'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac24b72b",
   "metadata": {},
   "source": [
    "<details><summary>Below is the code used for reading the files.</summary>\n",
    "\n",
    "    filepaths = ['data/train.txt', 'data/valid.txt', 'data/test.txt'] \n",
    "    given_entities = ['O', 'B-MISC', 'I-MISC', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']\n",
    "    merged_entities = ['O', 'MISC', 'PER', 'ORG', 'LOC'] \n",
    "    entity_map = {entity: i for i, entity in enumerate(given_entities)} \n",
    "\n",
    "    def readfile(filepath, sep=' '): \n",
    "        lines = open(filepath)\n",
    "\n",
    "        data, sentence, label = [], [], []\n",
    "        for line in lines:\n",
    "            if len(line) == 0 or line.startswith('-DOCSTART') or line[0] == '\\n':\n",
    "                if len(sentence) > 0:\n",
    "                    data.append((sentence, label))\n",
    "                    sentence, label = [], []\n",
    "                continue\n",
    "            splits = line.split(sep) \n",
    "            word = splits[0]\n",
    "            if len(word) > 0 and word[0].isalpha() and word.isupper():\n",
    "                word = word[0] + word[1:].lower()\n",
    "            sentence.append(word)\n",
    "            label.append(entity_map[splits[-1][:-1]])\n",
    "\n",
    "        if len(sentence) > 0:\n",
    "            data.append((sentence, label))\n",
    "\n",
    "        given_words = [d[0] for d in data] \n",
    "        given_labels = [d[1] for d in data] \n",
    "\n",
    "        return given_words, given_labels \n",
    "\n",
    "    given_words, given_labels = [], [] \n",
    "\n",
    "    for filepath in filepaths: \n",
    "        words, label = readfile(filepath) \n",
    "        given_words.extend(words) \n",
    "        given_labels.extend(label)\n",
    "\n",
    "    sentences = list(map(get_sentence, given_words)) \n",
    "\n",
    "    sentences, mask = filter_sentence(sentences) \n",
    "    given_words = [words for m, words in zip(mask, given_words) if m] \n",
    "    given_labels = [labels for m, labels in zip(mask, given_labels) if m] \n",
    "\n",
    "    maps = [0, 1, 1, 2, 2, 3, 3, 4, 4] \n",
    "    given_labels = [mapping(labels, maps) for labels in given_labels] \n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f632872e",
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "filepaths = ['data/train.txt', 'data/valid.txt', 'data/test.txt'] \n",
    "given_entities = ['O', 'B-MISC', 'I-MISC', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'] \n",
    "merged_entities = ['O', 'MISC', 'PER', 'ORG', 'LOC'] \n",
    "entity_map = {entity: i for i, entity in enumerate(given_entities)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202f1526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfile(filepath, sep=' '): \n",
    "    lines = open(filepath)\n",
    "    \n",
    "    data, sentence, label = [], [], []\n",
    "    for line in lines:\n",
    "        if len(line) == 0 or line.startswith('-DOCSTART') or line[0] == '\\n':\n",
    "            if len(sentence) > 0:\n",
    "                data.append((sentence, label))\n",
    "                sentence, label = [], []\n",
    "            continue\n",
    "        splits = line.split(sep) \n",
    "        word = splits[0]\n",
    "        if len(word) > 0 and word[0].isalpha() and word.isupper():\n",
    "            word = word[0] + word[1:].lower()\n",
    "        sentence.append(word)\n",
    "        label.append(entity_map[splits[-1][:-1]])\n",
    "\n",
    "    if len(sentence) > 0:\n",
    "        data.append((sentence, label))\n",
    "        \n",
    "    given_words = [d[0] for d in data] \n",
    "    given_labels = [d[1] for d in data] \n",
    "    \n",
    "    return given_words, given_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4381f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "given_words, given_labels = [], [] \n",
    "\n",
    "for filepath in filepaths: \n",
    "    words, label = readfile(filepath) \n",
    "    given_words.extend(words) \n",
    "    given_labels.extend(label)\n",
    "    \n",
    "sentences = list(map(get_sentence, given_words)) \n",
    "\n",
    "sentences, mask = filter_sentence(sentences) \n",
    "given_words = [words for m, words in zip(mask, given_words) if m] \n",
    "given_labels = [labels for m, labels in zip(mask, given_labels) if m] \n",
    "\n",
    "maps = [0, 1, 1, 2, 2, 3, 3, 4, 4] \n",
    "labels = [mapping(labels, maps) for labels in given_labels] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cb7c93",
   "metadata": {},
   "source": [
    "Here, we request the inputs to be in the following format: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7842e4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_to_preview = 3\n",
    "for i in range(indices_to_preview):\n",
    "    print('\\nsentences[%d]:\\t' % i + str(sentences[i])) \n",
    "    print('labels[%d]:\\t' % i + str(labels[i])) \n",
    "    print('pred_probs[%d]:\\n' % i + str(pred_probs[i])) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc3150f",
   "metadata": {},
   "source": [
    "## 3. Use cleanlab to find potential label issues \n",
    "\n",
    "Based on the given labels and out-of-sample predicted probabilities, cleanlab can quickly help us identify label issues in our dataset. Here we request that the indices of the identified label issues be sorted by cleanlab’s self-confidence score, which measures the quality of each given label via the probability assigned to it in our model’s prediction. Here, `issues` is a list of tuples `(i, j)`, which corresponds to the `j`'th token of the `i`'th sentence. These are the tokens cleanlab thinks may be badly labeled in your dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2ad9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "issues = find_label_issues(labels, pred_probs, return_indices_ranked_by='self_confidence') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7221c12b",
   "metadata": {},
   "source": [
    "## 4. Most likely issues \n",
    "Let's look at the top 20 examples cleanlab thinks are most likely to be incorrectly labeled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dc7268",
   "metadata": {},
   "outputs": [],
   "source": [
    "top = 20 \n",
    "print('Cleanlab found %d potential label issues. ' % len(issues)) \n",
    "print('The top %d most likely label errors:' % top) \n",
    "print(issues[:top]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65421a2d",
   "metadata": {},
   "source": [
    "We show the top 20 potential label issues. Given that `O` and `MISC` are hard to distinguish and can sometimes be ambiguous, they are excluded from the examples below. They can be specified in the `exclude` argument, which is a list of tuples `(i, j)` such that tokens predicted as `merged_entities[j]` but labels as `merged_entities[i]` are ignored. In the following example, we ignore mislabels between `O` and `MISC`, which are indexed `0` and `1`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13de188",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_issues(issues, given_words, pred_probs=pred_probs, given_labels=labels, \n",
    "               exclude=[(0, 1), (1, 0)], class_names=merged_entities) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d04902",
   "metadata": {},
   "source": [
    "More than half of the potential label issues correspond to tokens that are incorrectly labeled. As shown above, some examples are ambigious and require manual checking. Observe that there are some edge cases such as tokens simply being punctuations such as `/` and `(`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d213b2b2",
   "metadata": {},
   "source": [
    "## 4. Most common word-level token mislabels \n",
    "\n",
    "It may be useful to examine the most common word-level token mislabels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a006bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = common_label_issues(issues, given_words, \n",
    "                           labels=labels, \n",
    "                           pred_probs=pred_probs, \n",
    "                           class_names=merged_entities, \n",
    "                           exclude=[(0, 1), (1, 0)]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c417061",
   "metadata": {},
   "source": [
    "The above printed information is also stored as a DataFrame `info`, sorted by the number of mislabels in descending order. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35ef843",
   "metadata": {},
   "source": [
    "## 5. Find issue sentences with particular word \n",
    "\n",
    "Call `search_token` to examine the token label issues of a specific token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f4e163",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_issues = filter_by_token('United', issues, given_words) \n",
    "display_issues(token_issues, given_words, pred_probs=pred_probs, given_labels=labels, \n",
    "               exclude=[(0, 1), (1, 0)], class_names=merged_entities) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1759108b",
   "metadata": {},
   "source": [
    "## 6. Sentence label quality score \n",
    "\n",
    "Cleanlab can analyze every label in the dataset and provide a numerical score for each sentence. The score ranges between 0 and 1: a lower score indicates that the sentence is more likely to contain at least one error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0b5179",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, token_scores = get_label_quality_scores(labels, pred_probs) \n",
    "issues = issues_from_scores(scores, token_scores=token_scores) \n",
    "display_issues(issues, given_words, pred_probs=pred_probs, given_labels=labels, \n",
    "               exclude=[(0, 1), (1, 0)], class_names=merged_entities) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18795eb",
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "highlighted_indices = [(2907, 0), (19392, 0), (9962, 4), (8904, 30), (19303, 0), \n",
    "                       (12918, 0), (9256, 0), (11855, 20), (18392, 4), (20426, 28), \n",
    "                       (19402, 21), (14744, 15), (19371, 0), (4645, 2), (83, 9), \n",
    "                       (10331, 3), (9430, 10), (6143, 25), (18367, 0), (12914, 3)] \n",
    "\n",
    "if not all(x in issues for x in highlighted_indices):\n",
    "    raise Exception(\"Some highlighted examples are missing from ranked_label_issues.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
